---
title: "Genthat CRAN Experiment"
author: "Filip Krikava & Jan Vitek"
output:
  html_document:
    df_print: paged
    toc: true
    toc_depth: 2
    toc_float: true
    highlight: tango
---

```{r setup, include=FALSE}
library(genthat)
library(dplyr)
library(tidyr)
library(purrr)
library(readr)
library(stringr)
library(ggplot2)

knitr::opts_chunk$set(echo = TRUE, comment=NA)
output_dir <- "experiment"
```

```{r aux functions, include=FALSE}
load_coverage <- function(path) {
  package <- basename(path)
  tests_coverage_file <- file.path(path, "coverage.RDS")
  tests_file <- file.path(path, "tests.RDS")
  
  tests_coverage_df <- if (file.exists(tests_coverage_file)) readRDS(tests_coverage_file) else NA
  tests_df <- if (file.exists(tests_file)) readRDS(tests_file) else NA
  
  stats <- rep(NA, 6)
  trace_time <- NA
  tests_coverage <- NA
  tests_coverage_time <- NA
  
  if (is.data.frame(tests_coverage_df) && is.data.frame(tests_df)) {
    stats <- attr(tests_df, "stats")
    genthat_coverage_df <- attr(tests_df, "raw_coverage")
    coverage <- compute_coverage(tests_coverage_df, genthat_coverage_df)
    trace_time <- as.numeric(attr(tests_df, "time"), unit="secs")
  } 
  
  if (is.data.frame(tests_coverage_df)) {
    tests_coverage <- compute_coverage(tests_coverage_df)
    tests_coverage_time <- as.numeric(attr(tests_coverage_df, "time"), unit="secs")
  }
  
  data_frame(
    package,
    all=stats[1], 
    generated=stats[2], 
    ran=stats[3], 
    kept=stats[4], 
    coverage, 
    genthat_tests_time=stats[6], 
    trace_time,
    tests_coverage, 
    tests_coverage_time
  )
}

fmt <- function(x) format(round(x, 2), big.mark=",")
my_avg <- function(xs) str_c(fmt(mean(xs, na.rm=TRUE)), " (s=", fmt(sd(xs, na.rm=TRUE)), ", m=", fmt(median(xs, na.rm=TRUE)), ")")
```

```{r constants from the paper, include=FALSE}
PAPER_NPACKAGES <- 1547
PAPER_SLOC <- 1700000
PAPER_TEST_COVERAGE_SLOC <- 267113
PAPER_COVERAGE_SLOC <- 704450
PAPER_ACCURACY <- 0.8
```

This notebook processes the results from the CRAN experiment and reproduces the data from Section 4.
We present the results in comparison with the data included in the paper.
The numbers differs based on what packages were run.
The main idea is to demonstrate the usability of the tool.
It shall be able to increase code coverage given that there is some runnable code in the packages.

The prerendered results in the repository shows the values for running all the packages (`packages-all.txt`).

## Loading the results

First we load the data from packages that ran.

We need to find which packages succeeded:
```{r}
packages_all_df <- 
  data_frame(package_path=list.dirs(output_dir, full.names=TRUE, recursive=FALSE)) %>% 
  filter(
    file.exists(file.path(package_path, "coverage.RDS")), 
    file.exists(file.path(package_path, "tests.RDS"))
  ) %>%
  rowwise() %>%
  do(load_coverage(.$package_path))
```

Next, we load the code size.
We ran the [cloc](http://cloc.sourceforge.net/) tool in the docker image to compute the size of R source code in all the packages in `/CRAN` directory.
Each package has `sloc.csv` file in its root which a result of running:

```sh
cloc --quiet --csv %p/R | sed 's/,\".*$//' | tail -n +2 > %p/sloc.csv
```

where `%s` is the name of the project.
Since this is a static information that does not depend in anyway on our tool, all the files have been already generated during the image build.

```{r}
sloc <- map_dfr(packages_all_df$package, function(x) {
  # read the output of cloc in CSV, skip all but code column
  s <- read_csv(file.path("/CRAN", x, "sloc.csv"), col_types=cols("code"="i"))
  # sum the code per package
  data_frame(package=x, code=sum(s$code))
})
```

## Results

```{r, include=FALSE}
test_coverage_sloc <- sum(sloc$code) * (mean(packages_all_df$tests_coverage, na.rm=T)/100)
coverage_sloc <- sum(sloc$code) * (mean(packages_all_df$coverage, na.rm=T)/100)
accuracy <- sum(packages_all_df$ran)/sum(packages_all_df$all)
```

Together, this experiment ran `r length(readLines("packages.txt"))` packages (the number of lines in `packages.txt`) out of which `r nrow(packages_all_df)` packages completed.
This is `r fmt(nrow(packages_all_df)/PAPER_NPACKAGES*100)`% of the packages reported in the the paper.
These packages comprise of `r fmt(sum(sloc$code))` lines of code (excluding white space and comments), which is `r fmt(sum(sloc$code)/PAPER_SLOC*100)`% of the original experiment.
We selected the packages which had the original tracing time less than 2 minutes to keep the artifact time manageable.

### Summary

The average test code coverage is `r my_avg(packages_all_df$tests_coverage/100)`.
With genthat this is increased to `r my_avg(packages_all_df$coverage/100)`.
In terms of source lines of code (as reported in both the paper abstract and paper introduction), **genthat improved code coverage from the original rather low value of `r fmt(test_coverage_sloc)` lines to `r fmt(coverage_sloc)` lines**.
This is an increase of `r fmt(coverage_sloc/test_coverage_sloc*100)`%.
In the paper we reported `r fmt(PAPER_COVERAGE_SLOC/PAPER_TEST_COVERAGE_SLOC*100)`%.
The following figure compares the test code coverage with genthat and without:

```{r}
packages_all_df %>%
  mutate(Genthat=coverage/100, Default=tests_coverage/100) %>%
  gather(key="x", value="y", Genthat, Default) %>%
  ggplot(aes(factor(x, levels=c("Genthat", "Default")), y, fill=x)) + 
    geom_boxplot(alpha=.4) +
    theme_minimal() +
    scale_y_continuous(labels=scales::percent_format()) +
    guides(fill=FALSE) +
    labs(x="Tests", y="Code coverage")
```

The section 4 presents the results in 3 tables and two figures.
The major results are in Table 1, Table 2 and Figure 4 which are shown bellow.
Table 3 describes what are the cases of lost accuracy.
A part of the table requires manual inspection of the failures (how to get errors has been explained in the _overview_ notebook).
Therefore, we reports only the overall accuracy `r fmt(accuracy*100)`% which should be similar to `r fmt(PAPER_ACCURACY*100)`% reported in the paper (again, depends which set of packages was ran).
This of course depends on the package selection, yet in a reasonable number of packages the numbers shall be close.
Figure 5 with reverse dependencies is skipped because:

1. rerunning reverse dependencies for these packages will be highly time consuming,
2. we have already shown in the _overview_ how it is done on a single package `Rvmmin`,
3. the above experiment shows the strategy of running genthat in a batch.

### Table 1

```{r}
table1 <- tribble(
  ~Name,                      ~Overall,                            ~`Average per package`,
  "Traced unique calls",      fmt(sum(packages_all_df$all)),       my_avg(packages_all_df$all),
  "Generated tests",          fmt(sum(packages_all_df$generated)), my_avg(packages_all_df$generated),
  "Valid and Correct tests",  fmt(sum(packages_all_df$ran)),       my_avg(packages_all_df$ran),
  "Non-redundant tests",      fmt(sum(packages_all_df$kept)),      my_avg(packages_all_df$kept),
  "Ratio of reproduced test", fmt(accuracy),                       my_avg(packages_all_df$ran/packages_all_df$all)
)

table1
```

Here we should see similar accuracy results.
The number of unique traces shall help to understand the size of the packages.
Since they are rather small (tracing time is under 2 minutes), there are not many calls from which to extract tests.

### Figure 4

```{r}
figure4 <- function(df) {
  select(df, package, coverage, tests_coverage) %>%
    mutate(Default=tests_coverage/100, Genthat=coverage/100) %>%
    gather(key="key", value="value", Default, Genthat) %>%
    ggplot(aes(x=value, fill=factor(key, levels=c("Genthat", "Default")))) + 
      geom_histogram(alpha=.4, bins=100, colour="grey") + 
      scale_x_continuous(labels=scales::percent_format()) +
      theme_minimal() +
      theme(legend.position=c(.8,.75)) +
      labs(x="Code coverage", y="Number of packages", fill="Tests")
}
```

```{r}
packages_all_df %>%
  filter(tests_coverage > 0) %>%
  figure4() %>%
  print()
```

Comparing with the Figure in the paper, one can see a similar shape.
In this case we removed the packages that have 0 test coverage since they will dominate this graph (_cf._ bellow).
The reason is that we run smaller packages (tracing time under 2 minutes).
In general, these packages tend to have less code and less tests.
Similarly to what is in the paper, genthat succeeded to increase the code coverage.

For the sake of completeness, here is the same figure showing all the data:
```{r}
packages_all_df %>%
  figure4() %>%
  print()
```

Here we can see that most of the packages do not have any tests.
With genthat, only very few packages does not have any code coverage.

### Table 2

```{r}
table2 <- tribble(
  ~Name,                  ~`Average per package`,
  "Tracing",              my_avg(packages_all_df$trace_time),
  "Testing with genthat", my_avg(packages_all_df$genthat_tests_time),
  "Testing",              my_avg(packages_all_df$tests_coverage_time)
)

table2
```
